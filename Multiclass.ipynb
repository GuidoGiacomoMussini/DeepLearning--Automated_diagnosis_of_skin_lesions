{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NPqW7FJkbzl2",
        "7Idk4uBubHHh",
        "VthWPFDWelao",
        "dlKujdx4bokG",
        "NVYVT0oVeUbw",
        "vwql8RGjghfQ",
        "wQjXLzN8hf5B",
        "NhCFAP4Gz3fe",
        "uZnbJ7KVjKYm",
        "XPl-a5FShCvA",
        "eQMZhgNduGjc",
        "cuTdfs7ejdDo",
        "Kfe4tzKyMswD",
        "cJp2zVZ7BuUy",
        "bv6WFjGCC_cs",
        "M9qD5mnwBGNp",
        "79bxtn6A-UD9",
        "FNVmSe_OOdta",
        "ntMxfsO0HTHl",
        "vyFRBdEQgAKR",
        "aUioboOikpcy",
        "fqnRjTlZ_rb_",
        "YfGckO_KcLUQ"
      ],
      "authorship_tag": "ABX9TyPYIY9vkCIfSInhaLb5UuAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuidoGiacomoMussini/DeepLearning-automated_diagnosis_of_pigmented_skin_lesions/blob/main/Multiclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Skin Lesion Project - Notebook 1**\n",
        "\n",
        "---\n",
        "\n",
        "##7 Classes Classification:\n",
        "\n",
        "* akiec      \n",
        "* bcc     \n",
        "* bkl \n",
        "* df    \n",
        "* mel   \n",
        "* nv \n",
        "* vasc     \n",
        "     \n",
        "\n",
        "---\n",
        "Guido Giacomo Mussini 988273\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hH4j--WtAEb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0 - Initialization**"
      ],
      "metadata": {
        "id": "NPqW7FJkbzl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 - Libraries "
      ],
      "metadata": {
        "id": "7Idk4uBubHHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "id": "ZpMTOffUOTeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb876d6-40b4-4727-f8b3-58e4554b65b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import random as rnd\n",
        "from IPython.display import clear_output \n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from collections import defaultdict, Counter, OrderedDict\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchmetrics.classification import MulticlassAccuracy, BinaryAccuracy\n",
        "from torchmetrics.classification import MulticlassConfusionMatrix\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools"
      ],
      "metadata": {
        "id": "8P8eROGcbOVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 - Functions"
      ],
      "metadata": {
        "id": "VthWPFDWelao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model prediction: \n",
        "\n",
        "* train the model and test the performance.\n",
        "* Input: train and test set, number of epochs, the loss function, the optimizer and a boolean value which shows the progression of the algorithm in the epochs\n",
        "* It returns 2 dictionaries containing information about Accuracy and Loss of training and test set\n",
        "* inspired by: https://github.com/dash-ka/DL_Natural_Language_Processing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2EiPcpD1eqDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Prediction(model, train_loader, test_loader, n_epochs, criterion, optimizer, show_progress): \n",
        "\n",
        "  model_name = model.__class__.__name__\n",
        "  loss_dict, acc_dict = defaultdict(list), defaultdict(list)\n",
        "  tr_acc, tr_tot, train_accuracy, val_acc, val_tot, validation_accuracy = 0,0,0,0,0,0\n",
        "\n",
        "  for epoch in tqdm_notebook(range(n_epochs)):\n",
        "\n",
        "    rtrain_loss = 0\n",
        "    model.train()\n",
        "    #train\n",
        "    for batch_index, tr_batch in enumerate(train_loader, start = 1):\n",
        "\n",
        "        #retrieve  the data\n",
        "        images, labels = tr_batch\n",
        "        images = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "\n",
        "        #forword step\n",
        "        y_pred = model(images)\n",
        "        #loss \n",
        "        loss = criterion(y_pred, labels)\n",
        "        #backprop\n",
        "        loss.backward()\n",
        "        #param updates\n",
        "        optimizer.step() \n",
        "        # clear the gradient \n",
        "        model.zero_grad()\n",
        "\n",
        "        # save results\n",
        "        #train_loss.append(loss.item())\n",
        "        batch_loss = loss.item()\n",
        "        rtrain_loss += (batch_loss - rtrain_loss) / batch_index  \n",
        "        #accuracy\n",
        "        _, predicted = torch.max(y_pred, 1)\n",
        "        tr_tot += labels.size(0)\n",
        "        tr_acc += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    train_accuracy = (tr_acc)/tr_tot\n",
        "    acc_dict[\"training_accuracy\"].append(train_accuracy)\n",
        "    loss_dict[\"training_loss\"].append(rtrain_loss)\n",
        "\n",
        "    #update the gradient modifier\n",
        "    scheduler.step(rtrain_loss)\n",
        "\n",
        "    model.eval()\n",
        "    rval_loss = 0\n",
        "    #eval\n",
        "    for batch_index, val_batch in enumerate(test_loader, start = 1):\n",
        "\n",
        "      #get the data\n",
        "      images, labels = val_batch\n",
        "      images = Variable(images).to(device)\n",
        "      labels = Variable(labels).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        #prediction\n",
        "        val_pred = model(images)\n",
        "        #prediction loss\n",
        "        v_loss = criterion(val_pred, labels)\n",
        "\n",
        "        #save results\n",
        "        #val_loss.append(loss.item())\n",
        "        batch_loss = v_loss.item()\n",
        "        rval_loss += (batch_loss - rval_loss) / batch_index \n",
        "        #accuracy\n",
        "        _, predicted = torch.max(val_pred, 1)\n",
        "        val_tot += labels.size(0)\n",
        "        val_acc += (predicted == labels).sum().item()\n",
        "\n",
        "    validation_accuracy = (val_acc)/val_tot\n",
        "    acc_dict[\"validation_accuracy\"].append(validation_accuracy)\n",
        "    loss_dict[\"validation_loss\"].append(rval_loss)\n",
        "    \n",
        "    if show_progress == True:\n",
        "      print('[Model: %s ] -> [epoch %d]: \\n [train Loss %.5f], [val Loss %.5f] \\n [train Acc  %.5f], [val Acc  %.5f]' \\\n",
        "            % (model_name, epoch, rtrain_loss, rval_loss, train_accuracy, validation_accuracy ))\n",
        "      print(\"---------------------------\")\n",
        "\n",
        "  return loss_dict, acc_dict"
      ],
      "metadata": {
        "id": "QPFrU24-ae5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on the train set\n",
        "\n",
        "* inspired by: https://www.kaggle.com/code/unstructuredrahul/deep-learning-pytorch-binary-classification?scriptVersionId=67067211&cellId=38"
      ],
      "metadata": {
        "id": "z-DEmqjk8mgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pred(model, test_loader):\n",
        "  with torch.no_grad():\n",
        "\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for i, data in enumerate(test_loader):\n",
        "\n",
        "      images, labels = data\n",
        "      images = Variable(images)\n",
        "      labels = Variable(labels)\n",
        "      labels = labels.float()\n",
        "\n",
        "      y_test_pred = model(images)\n",
        "      y_test_pred = y_test_pred.argmax(axis=1)\n",
        "      preds.append(y_test_pred.detach().numpy())\n",
        "\n",
        "  return preds"
      ],
      "metadata": {
        "id": "FpdmW_rw8eaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function used to fix the indexes of the dataset"
      ],
      "metadata": {
        "id": "V0VU3wBc8x0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_index(data):\n",
        "  s = pd.Series(range(len(data)))\n",
        "  data = data.set_index(s)\n",
        "  return data"
      ],
      "metadata": {
        "id": "mme2MFfpR3nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the accuracy of the test prediction"
      ],
      "metadata": {
        "id": "knBd8m3E83qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Accuracy(Dataframe):\n",
        "  Mat = Dataframe.to_numpy()\n",
        "  TP = 0\n",
        "  Tot = 0\n",
        "  for i in range(0,7):\n",
        "    for j in range(0,7):\n",
        "      Tot = Mat[i][j] + Tot\n",
        "      if i == j: TP = Mat[i][j] + TP\n",
        "  return TP/Tot"
      ],
      "metadata": {
        "id": "Z7CfP-43fKKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 - Import the Data From Kaggle (estimated time: 4 m)"
      ],
      "metadata": {
        "id": "dlKujdx4bokG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir WD\n",
        "\n",
        "! #mkdir ~/.kaggle\n",
        "! #cp kaggle.json ~/.kaggle/\n",
        "! #chmod 600 ~/.kaggle/kaggle.json\n",
        "os.environ['KAGGLE_USERNAME'] = \"guidomussini\"\n",
        "os.environ['KAGGLE_KEY'] = \"f7b24d630bc3e7e7fda7a5a1b32f4582\"\n",
        "! kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "! unzip /content/skin-cancer-mnist-ham10000.zip -d /content/WD\n",
        "\n",
        "#remove useless data \n",
        "shutil.rmtree('/content/WD/ham10000_images_part_1')\n",
        "shutil.rmtree('/content/WD/ham10000_images_part_2')\n",
        "! rm '/content/WD/hmnist_28_28_L.csv'\n",
        "! rm '/content/WD/hmnist_28_28_RGB.csv'\n",
        "! rm '/content/WD/hmnist_8_8_L.csv'\n",
        "! rm '/content/WD/hmnist_8_8_RGB.csv'\n",
        "! rm '/content/skin-cancer-mnist-ham10000.zip'"
      ],
      "metadata": {
        "id": "WXNeJUwkcFF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Define the dataset**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NVYVT0oVeUbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 - Merge the images from the 2 folders\n",
        "\n",
        "*   Code taken by: https://www.kaggle.com/code/sid321axn/step-wise-approach-cnn-model-77-0344-accuracy\n"
      ],
      "metadata": {
        "id": "vwql8RGjghfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the images of the 2 folders"
      ],
      "metadata": {
        "id": "FUdinGUh9P6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_skin_dir = os.path.join('..', 'content/WD')\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n",
        "\n",
        "sdf = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))"
      ],
      "metadata": {
        "id": "W_ns6GX0e3Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a column in which each row contain the path to a image"
      ],
      "metadata": {
        "id": "bGoBtAQEg3ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n",
        "sdf['path'] = sdf['image_id'].map(imageid_path_dict.get)"
      ],
      "metadata": {
        "id": "7njCNeyVgwSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Metadata Handling"
      ],
      "metadata": {
        "id": "wQjXLzN8hf5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for duplicate images"
      ],
      "metadata": {
        "id": "y35kfzgAhsqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = sdf.drop_duplicates(subset=['lesion_id']) \n",
        "sdf = sdf.drop_duplicates(subset=['image_id']) \n",
        "\n",
        "print(sdf.shape[0]) #now i have 7470 lesions "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdNpVAajhm1w",
        "outputId": "b4b16e7b-3a3e-44a0-f169-f2c5d9ebc430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for Missing Values"
      ],
      "metadata": {
        "id": "oAFKixXRhz5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of non-NA data per columns:\\n\",sdf.isnull().sum())\n",
        "#some NA in age -> since that column will be removed from the dataset, i don't impute them"
      ],
      "metadata": {
        "id": "NQJrjjnGh4nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3eb7a18-5dad-4644-f966-4b0049b50908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of non-NA data per columns:\n",
            " lesion_id        0\n",
            "image_id         0\n",
            "dx               0\n",
            "dx_type          0\n",
            "age             52\n",
            "sex              0\n",
            "localization     0\n",
            "path             0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Data Visualisation**"
      ],
      "metadata": {
        "id": "NhCFAP4Gz3fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lesion distribution: \n",
        "* **green** if the lesion is **benign**\n",
        "* **red** if the lesion is **malignant**"
      ],
      "metadata": {
        "id": "m1mns3FzUiSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_perc = round((sdf[\"dx\"].value_counts() / len(sdf[\"dx\"])), 2)\n",
        "#nv       0.72\n",
        "#bkl      0.10\n",
        "#mel      0.08\n",
        "#bcc      0.04\n",
        "#akiec    0.03\n",
        "#vasc     0.01\n",
        "#df       0.01\n",
        "plt.bar(class_perc.index, class_perc, color = [\"green\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\"])"
      ],
      "metadata": {
        "id": "_RPggc610pT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Prepare the Data**"
      ],
      "metadata": {
        "id": "uZnbJ7KVjKYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Generate the Training (train + val) and Test set"
      ],
      "metadata": {
        "id": "XPl-a5FShCvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the response variable"
      ],
      "metadata": {
        "id": "WMbJP-mHt6c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = re_index(sdf)\n",
        "sdf['label'] = pd.Categorical(sdf['dx']).codes\n",
        "sdf = sdf[['path', 'label']]\n",
        "\n",
        "#this function, defined in the section 'functions', fix the row-index of the dataset\n",
        "sdf = re_index(sdf)"
      ],
      "metadata": {
        "id": "eeQU8vtUtPbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample the training and the test set"
      ],
      "metadata": {
        "id": "DlazThHfuBRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_val, test = train_test_split(sdf, test_size=0.30,random_state=19)\n",
        "#need to convert x_train_val in an array to merge it in the future with the augumented data\n",
        "x_train_val = train_val[['path']].to_numpy() \n",
        "\n",
        "#re index the df \n",
        "train_val = re_index(train_val)\n",
        "test = re_index(test)\n",
        "\n",
        "print(\"Training set:\", len(train_val), \"\\nTest set:\", len(test))"
      ],
      "metadata": {
        "id": "PvZC1qHRtPhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4adca0f6-c33f-4326-d052-26b63ccd2558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 5229 \n",
            "Test set: 2241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Store the images as arrays (Expected time: 3m)\n",
        " \n",
        " The images are sized as 32x32 since speed up the computation of the algorithm mantaining a good amount of information"
      ],
      "metadata": {
        "id": "eQMZhgNduGjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sh_x = 32\n",
        "sh_y = 32\n",
        "train_val['image'] = train_val['path'].map(lambda x: np.asarray(Image.open(x).resize((sh_x,sh_y))))\n",
        "test['image'] = test['path'].map(lambda x: np.asarray(Image.open(x).resize((sh_x,sh_y))))\n",
        "\n",
        "#delete useless variables\n",
        "train_val = train_val[['image', 'label']]\n",
        "test = test[['image', 'label']]\n",
        "\n",
        "#trasform in array for the next steps\n",
        "x_train_val = np.asarray(train_val['image'].tolist())\n",
        "y_train = np.array(train_val['label'])\n"
      ],
      "metadata": {
        "id": "5UubIT-2tPjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 - Data Augumentation\n",
        "\n",
        "Since the training set contain around 5000 observations, a good practice is to generate synthetic images in order to increase the number of observation. \n",
        "\n",
        "I have a random mix of vertical and horizontal flip to augument the data. In that way the dataset cardinality is doubled\n",
        "\n",
        "**Note that** the augumented data have been used only in the training set (train +validation), while in the test set has been used only original data."
      ],
      "metadata": {
        "id": "cuTdfs7ejdDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug = list()\n",
        "#T.RandomPerspective(distortion_scale=0.4, p=1.0), \\\n",
        "transforms = T.RandomApply(torch.nn.ModuleList([T.RandomVerticalFlip(p=1.0), \\\n",
        "                                                T.RandomHorizontalFlip(p=1.0)]), p=1)\n",
        "\n",
        "#augumenter = T.RandomPerspective(distortion_scale=0.6, p=1.0)\n",
        "aug = train_val[\"image\"].map(lambda x: transforms(torch.tensor(x).permute(2,0,1)))\n",
        "\n",
        "for i in range(len(aug)):\n",
        "  aug[i] = np.array(aug[i])"
      ],
      "metadata": {
        "id": "sO5YiMIcujZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataset containing the original data and the augumented ones"
      ],
      "metadata": {
        "id": "26sJrb8eAKq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#trasform the augumented data as an array\n",
        "aug = np.asarray(aug.tolist())\n",
        "aug = aug.reshape(aug.shape[0], *(sh_x, sh_y, 3))\n",
        "\n",
        "#trasform the images into array\n",
        "x_train_val = np.asarray(x_train_val.tolist())\n",
        "x_train_val = x_train_val.reshape(x_train_val.shape[0], *(sh_x, sh_y, 3))"
      ],
      "metadata": {
        "id": "km0VIIWlR0OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenete the 2 arrays\n",
        "df_images = np.concatenate((aug, x_train_val))\n",
        "\n",
        "#double the labels to make them fit with the original data and the concatenated augumented data\n",
        "df_labels = np.concatenate((y_train, y_train))\n",
        "#df_labels = np.concatenate((y_train_Smote, y_train))\n",
        "\n",
        "#define the dataset containing training and val examples\n",
        "coln1 = {'image': list(df_images), 'label': list(df_labels)}\n",
        "trainval = pd.DataFrame(data=coln1)\n",
        "\n",
        "print(\"len training:\", len(df_labels))"
      ],
      "metadata": {
        "id": "G6eeeXa45PRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13576989-ee42-4a75-cd5c-4c2068b34bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len training: 10458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 - Split the training set in Train and Validation set"
      ],
      "metadata": {
        "id": "Kfe4tzKyMswD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(trainval, test_size=0.30,random_state=19)\n",
        "#fix the indexes\n",
        "train = re_index(train)\n",
        "val = re_index(val)\n",
        "print(\"train:\", len(train), \"\\nval:\", len(val))"
      ],
      "metadata": {
        "id": "XubzKXVG5RS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7670e93-c1c8-48e2-8e40-b12e65118571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 7320 \n",
            "val: 3138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 - Define features and response variable for train, validation and test set"
      ],
      "metadata": {
        "id": "cJp2zVZ7BuUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "x_train = train.drop([\"label\"], axis = 1)\n",
        "x_train = np.asarray(x_train['image'].tolist())\n",
        "y_train = train[\"label\"]\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "#validation\n",
        "x_val = val.drop([\"label\"], axis = 1)\n",
        "x_val = np.asarray(x_val['image'].tolist())\n",
        "y_val = val[\"label\"]\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "#test\n",
        "x_test = test.drop([\"label\"], axis = 1)\n",
        "x_test = np.asarray(x_test['image'].tolist())\n",
        "y_test = test[\"label\"]\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "uavM1nqz5Tp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 - Reshape and convert the images in the correct format\n",
        "\n",
        "all the images have been shaped as 32x32x3 images"
      ],
      "metadata": {
        "id": "bv6WFjGCC_cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape to be sure that they are in the right shape\n",
        "x_train = x_train.reshape(x_train.shape[0], *(sh_x, sh_y, 3))\n",
        "x_val = x_val.reshape(x_val.shape[0], *(sh_x, sh_y, 3))\n",
        "x_test = x_test.reshape(x_test.shape[0], *(sh_x, sh_y, 3))"
      ],
      "metadata": {
        "id": "72gCZO3h5Z65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the images in torch format\n",
        "x_train  =torch.from_numpy(x_train.astype(np.float32))\n",
        "x_val  = torch.from_numpy(x_val.astype(np.float32))\n",
        "x_test  = torch.from_numpy(x_test.astype(np.float32))\n",
        "\n",
        "#converting the lables in torch format\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_val = torch.from_numpy(y_val.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n"
      ],
      "metadata": {
        "id": "EZQRx_7q5eWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 - Normalization \n",
        "\n",
        "All the images are normalized"
      ],
      "metadata": {
        "id": "M9qD5mnwBGNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = TF.normalize(x_train, x_train.mean(), x_train.std())\n",
        "x_val = TF.normalize(x_val, x_val.mean(), x_val.std())\n",
        "x_test = TF.normalize(x_test, x_test.mean(), x_test.std())"
      ],
      "metadata": {
        "id": "OiFWMuhaBGf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 - CNN Framework**\n"
      ],
      "metadata": {
        "id": "79bxtn6A-UD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Pytorch Dataset and Dataloader"
      ],
      "metadata": {
        "id": "FNVmSe_OOdta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use GPU if possible"
      ],
      "metadata": {
        "id": "6OAyqmQROaB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "aKbwru9NF8zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ce1e91-b304-4eeb-a7f9-94d44a1681f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Class Data"
      ],
      "metadata": {
        "id": "vKM8IKr-IyvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.permute(0,3,1,2)\n",
        "        self.y = y.type(torch.LongTensor)\n",
        "        self.len = self.X.shape[0]\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "   \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "9Xt8Y2RY-aBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define train, validation and test loader"
      ],
      "metadata": {
        "id": "NxlLjZ4CKr8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 \n",
        "\n",
        "train_set = Data(x_train, y_train)\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_set = Data(x_val, y_val)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_set = Data(x_test, y_test)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "F0izQRsaI6H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2- Model1"
      ],
      "metadata": {
        "id": "ntMxfsO0HTHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=(5, 5))\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(3, 3))\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.drop40 = nn.Dropout(0.2)\n",
        "        self.drop20 = nn.Dropout(0.5)\n",
        "        self.LSM= torch.nn.LogSoftmax(dim=1)\n",
        "        \n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d3 = nn.BatchNorm2d(128)\n",
        " \n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        self.linear1 = nn.Linear(128, 64)\n",
        "        self.norm1d1 = nn.BatchNorm1d(64)\n",
        "        self.linear2 = nn.Linear(64, 42)\n",
        "        self.norm1d2 = nn.BatchNorm1d(42)\n",
        "        self.linear3 = nn.Linear(42, 7)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "  \n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.pool5(x)\n",
        "        x = self.norm2d1(x)\n",
        "        x = self.drop40(x)\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.norm2d2(x)\n",
        "        x = self.drop40(x)\n",
        "\n",
        "        x = self.ReLU(self.conv3(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.norm2d3(x)\n",
        "        x = self.drop40(x)\n",
        "\n",
        "        x = self.flat(x)\n",
        "\n",
        "        x = self.ReLU(self.linear1(x))\n",
        "        x = self.norm1d1(x)\n",
        "        x = self.drop20(x)\n",
        "\n",
        "        x = self.ReLU(self.linear2(x))\n",
        "        x = self.norm1d2(x)\n",
        "        x = self.drop20(x)\n",
        "\n",
        "        x = self.LSM(self.linear3(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "cgLnwuMIHQri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 - ModelN\n",
        "\n",
        "* Based on: https://www.nature.com/articles/s41598-022-22644-9#Tab2"
      ],
      "metadata": {
        "id": "vyFRBdEQgAKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(3, 3))\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(3, 3))\n",
        "        self.drop25 = nn.Dropout(0.25)\n",
        "        self.drop50 = nn.Dropout(0.50)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.LSM= torch.nn.LogSoftmax(dim=1)\n",
        "        \n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm3 = nn.BatchNorm2d(128)\n",
        " \n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        self.linear1 = nn.Linear(576, 1024)\n",
        "        self.normL1 = nn.BatchNorm1d(1024)\n",
        "        self.linear2 = nn.Linear(1024, 256)\n",
        "        self.normL2 = nn.BatchNorm1d(256)\n",
        "        self.linear3 = nn.Linear(256, 42)\n",
        "        self.normL3 = nn.BatchNorm1d(42)\n",
        "        self.linear4= nn.Linear(42, 7)\n",
        " \n",
        "    def forward(self, x):\n",
        "  \n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.drop25(x)\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.drop25(x)\n",
        "\n",
        "        x = self.flat(x)\n",
        "\n",
        "        x = self.ReLU(self.linear1(x))\n",
        "        x = self.normL1(x)\n",
        "        x = self.drop50(x)\n",
        "\n",
        "        x = self.ReLU(self.linear2(x))\n",
        "        x = self.normL2(x)\n",
        "        x = self.drop50(x)\n",
        "\n",
        "        x = self.ReLU(self.linear3(x))\n",
        "        x = self.normL3(x)\n",
        "        x = self.drop50(x)\n",
        "\n",
        "        x = self.LSM(self.linear4(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "eerP659ngJyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 - Model Training"
      ],
      "metadata": {
        "id": "aUioboOikpcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ModelN"
      ],
      "metadata": {
        "id": "z8edNNtHKq4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "model = ModelN() #Model1\n",
        "lr = 0.01\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    factor=0.1, \n",
        "    patience=10, \n",
        "    verbose=True)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "lossN, accN = Prediction(model, train_loader, val_loader, n_epochs, criterion, optimizer, show_progress = False)"
      ],
      "metadata": {
        "id": "iSwwvsCid0P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model1 "
      ],
      "metadata": {
        "id": "011ugtuwAAVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "model = Model1()\n",
        "lr = 0.01\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    factor=0.1, \n",
        "    patience=10, \n",
        "    verbose=True)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "loss1, acc1 = Prediction(model, train_loader, val_loader, n_epochs, criterion, optimizer, show_progress = True)"
      ],
      "metadata": {
        "id": "aydmgQMljFNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print the summary of the model"
      ],
      "metadata": {
        "id": "Kc3CHtRn_kiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#summary(model, (3,32,32))"
      ],
      "metadata": {
        "id": "1uDA3HV9oyP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Result visualization"
      ],
      "metadata": {
        "id": "fqnRjTlZ_rb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "JWU4UzANKszH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(acc1[\"training_accuracy\"], label = \"training accuracy\") #acc1\n",
        "plt.plot(acc1[\"validation_accuracy\"], label = \"validation accuracy\") #acc1\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t29m9EL5QqC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss"
      ],
      "metadata": {
        "id": "cNfhWnn-KvWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss1[\"training_loss\"], label = \"training loss\") #loss1\n",
        "plt.plot(loss1[\"validation_loss\"], label = \"validation loss\") #loss1\n",
        "plt.title(\"Loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rm4TP9NiKw2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 - Prediction on the Test set"
      ],
      "metadata": {
        "id": "YfGckO_KcLUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = test_pred(model, test_loader)"
      ],
      "metadata": {
        "id": "RwBqbsRlLGuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trasform the data to build the confusion matrix"
      ],
      "metadata": {
        "id": "P7WKWmMOA72z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_list = [a.squeeze().tolist() for a in preds]\n",
        "\n",
        "#last batch has only one element --> i temporary remove it to apply vstack.\n",
        "y_pred_list1 = y_pred_list[:-1]\n",
        "last_element = y_pred_list[-1]\n",
        "\n",
        "p = np.vstack(y_pred_list1)\n",
        "ytest_pred = list(itertools.chain.from_iterable(p))\n",
        "\n",
        "#append the last element\n",
        "ytest_pred.insert(len(ytest_pred), last_element)\n",
        "\n",
        "\n",
        "y_true_test = y_test.ravel()"
      ],
      "metadata": {
        "id": "3-OSxx9uLRXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "DGSRliH3BHqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = conf_matrix = confusion_matrix(y_true_test, ytest_pred)\n",
        "axis = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n",
        "adf = pd.DataFrame(a, columns = axis)\n",
        "adf.index = axis\n",
        "sns.heatmap(adf, annot=True, fmt = 'g', cmap=\"Blues\")"
      ],
      "metadata": {
        "id": "Ibu3V7F3LVWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "z2OoLrynA5Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy(adf)"
      ],
      "metadata": {
        "id": "cjw4y0-mA39a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}