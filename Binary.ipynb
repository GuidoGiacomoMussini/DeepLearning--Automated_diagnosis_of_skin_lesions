{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuidoGiacomoMussini/DeepLearning-automated_diagnosis_of_pigmented_skin_lesions/blob/main/Binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA75y-aPDW4E"
      },
      "source": [
        "\n",
        "# **Skin Lesion Project - Notebook 2**\n",
        "\n",
        "---\n",
        "## - Data Augumentation\n",
        "\n",
        "## - SMOTE \n",
        "\n",
        "##- Binary Classification:\n",
        "\n",
        "* Benign      \n",
        "* Malignant  \n",
        "\n",
        "## - Dropout Tuning\n",
        "\n",
        "---\n",
        "Guido Giacomo Mussini 988273\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTwd-q_DDg0m"
      },
      "source": [
        "# **0 - Initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6cncw0HDlx0"
      },
      "source": [
        "## 0.1 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMnZVGLC3VsN"
      },
      "outputs": [],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcVx5PjJ_MDj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import random as rnd\n",
        "from IPython.display import clear_output \n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from collections import defaultdict, Counter, OrderedDict\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchmetrics.classification import MulticlassAccuracy, BinaryAccuracy\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fS1sJvrDo38"
      },
      "source": [
        "## 0.2 Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnLix6tfDwNa"
      },
      "source": [
        "Model prediction: \n",
        "\n",
        "* train the model and test the performance.\n",
        "* Input: train and test set, number of epochs, the loss function and the optimizer. Show progress is a boolean that, if true, show the accuracy and the loss epoch by epoch\n",
        "* It returns 2 dictionaries containing information about Accuracy and Loss of training and test set\n",
        "* inspired by: https://github.com/dash-ka/DL_Natural_Language_Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGPGhWRK9sHA"
      },
      "outputs": [],
      "source": [
        "def Prediction(model, train_loader, test_loader, n_epochs, criterion, optimizer, show_progress):\n",
        "\n",
        "  model_name = model.__class__.__name__\n",
        "  loss_dict, acc_dict = defaultdict(list), defaultdict(list)\n",
        "  tr_acc, tr_tot, train_accuracy, val_acc, val_tot, validation_accuracy = 0,0,0,0,0,0\n",
        "\n",
        "  for epoch in tqdm_notebook(range(n_epochs)):\n",
        "      #Within each epoch run the subsets of data = batch sizes.\n",
        "      rtrain_loss = 0\n",
        "      model.train()\n",
        "      for b_index, t_batch in enumerate(train_loader, start = 1):\n",
        "\n",
        "        images, labels = t_batch\n",
        "        images = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "        labels = labels.float()\n",
        "\n",
        "        y_pred = model(images) \n",
        "        y_pred_tag = torch.round(y_pred)          \n",
        "        tloss = criterion(y_pred, labels)  \n",
        "        optimizer.zero_grad()         \n",
        "        tloss.backward()               \n",
        "        optimizer.step()\n",
        "\n",
        "        #loss\n",
        "        batch_loss = tloss.item()\n",
        "        rtrain_loss += (batch_loss - rtrain_loss) / b_index\n",
        "        \n",
        "\n",
        "        #accuracy\n",
        "        tr_acc += (y_pred_tag == labels).sum().item()\n",
        "        tr_tot += labels.size(0)   \n",
        "\n",
        "        #adaptive Learning rate\n",
        "        \n",
        "      train_accuracy = tr_acc / tr_tot\n",
        "      acc_dict[\"training_accuracy\"].append(train_accuracy)\n",
        "      loss_dict[\"training_loss\"].append(rtrain_loss)\n",
        "      scheduler.step(rtrain_loss)\n",
        "            \n",
        "\n",
        "      model.eval()\n",
        "      rval_loss = 0\n",
        "      with torch.no_grad():\n",
        "        for b_index, v_batch  in enumerate(test_loader, start = 1):\n",
        "\n",
        "            images, labels = v_batch\n",
        "            images = Variable(images).to(device)\n",
        "            labels = Variable(labels).to(device)\n",
        "            labels = labels.float()\n",
        "\n",
        "            y_test_pred = model(images)\n",
        "            y_pred_tag = torch.round(y_test_pred)\n",
        "            #y_pred_list.append(y_pred_tag.detach().numpy())\n",
        "            vloss = criterion(y_test_pred, labels)\n",
        "            \n",
        "            #loss\n",
        "            batch_loss = vloss.item()\n",
        "            rval_loss += (batch_loss - rval_loss) / b_index \n",
        "            \n",
        "            #acc\n",
        "            val_acc += (y_pred_tag == labels).sum().item()\n",
        "            val_tot += labels.size(0)\n",
        "\n",
        "        validation_accuracy = val_acc / val_tot\n",
        "        acc_dict[\"validation_accuracy\"].append(validation_accuracy) \n",
        "        loss_dict[\"validation_loss\"].append(rval_loss) \n",
        "\n",
        "      if show_progress == True:\n",
        "        print('[Model: %s ] -> [epoch %d]: \\n [train Loss %.5f], [val Loss %.5f] \\n [train Acc  %.5f], [val Acc  %.5f]' \\\n",
        "              % (model_name, epoch, rtrain_loss, rval_loss, train_accuracy, validation_accuracy ))\n",
        "        print(\"---------------------------------\")\n",
        "  return loss_dict, acc_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to compute the prediction on the test set\n",
        "* inspired by: https://www.kaggle.com/code/unstructuredrahul/deep-learning-pytorch-binary-classification?scriptVersionId=67067211&cellId=38"
      ],
      "metadata": {
        "id": "z7scFL9XDFYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction(model, test_loader):\n",
        "  with torch.no_grad():\n",
        "\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for i, data in enumerate(test_loader):\n",
        "\n",
        "      images, labels = data\n",
        "      images = Variable(images).to(device)\n",
        "      labels = Variable(labels).to(device)\n",
        "      labels = labels.float()\n",
        "\n",
        "      y_test_pred = model(images)\n",
        "      y_test_pred = y_test_pred.round()\n",
        "      preds.append(y_test_pred.detach().numpy())\n",
        "  return preds"
      ],
      "metadata": {
        "id": "hb3RCk1SDFwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9WmMIbd4sQP"
      },
      "source": [
        "Function to fix the row-index of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQRuVn3I6Rvs"
      },
      "outputs": [],
      "source": [
        "def re_index(data):\n",
        "  s = pd.Series(range(len(data)))\n",
        "  data = data.set_index(s)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DGUb17oECQ_"
      },
      "source": [
        "## 0.3 - Import the Data From Kaggle (estimated time: 4 m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VERqEz_q_Vzz"
      },
      "outputs": [],
      "source": [
        "!mkdir WD\n",
        "\n",
        "! #mkdir ~/.kaggle\n",
        "! #cp kaggle.json ~/.kaggle/\n",
        "! #chmod 600 ~/.kaggle/kaggle.json\n",
        "os.environ['KAGGLE_USERNAME'] = \"guidomussini\"\n",
        "os.environ['KAGGLE_KEY'] = \"f7b24d630bc3e7e7fda7a5a1b32f4582\"\n",
        "! kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "! unzip /content/skin-cancer-mnist-ham10000.zip -d /content/WD\n",
        "\n",
        "#remove useless data \n",
        "shutil.rmtree('/content/WD/ham10000_images_part_1')\n",
        "shutil.rmtree('/content/WD/ham10000_images_part_2')\n",
        "! rm '/content/WD/hmnist_28_28_L.csv'\n",
        "! rm '/content/WD/hmnist_28_28_RGB.csv'\n",
        "! rm '/content/WD/hmnist_8_8_L.csv'\n",
        "! rm '/content/WD/hmnist_8_8_RGB.csv'\n",
        "! rm '/content/skin-cancer-mnist-ham10000.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQfxsIBCEIje"
      },
      "source": [
        "# **1 - Dataset Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLnL3_d6EPKR"
      },
      "source": [
        "##1.1 - Merge the images from the 2 folders\n",
        "\n",
        "*   Code taken by: https://www.kaggle.com/code/sid321axn/step-wise-approach-cnn-model-77-0344-accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9_InBsY_Zu3"
      },
      "outputs": [],
      "source": [
        "#Merge the images of the 2 folders\n",
        "base_skin_dir = os.path.join('..', 'content/WD')\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n",
        "\n",
        "sdf = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ci7zPVEWiQ"
      },
      "source": [
        "create a column in which each row contain the path to a image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na3AfpSe_dUD"
      },
      "outputs": [],
      "source": [
        "sdf = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n",
        "\n",
        "sdf['path'] = sdf['image_id'].map(imageid_path_dict.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE5m2sGVEg6O"
      },
      "source": [
        "## 1.2 - Metadata Handling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL5-DrjhEj_V"
      },
      "source": [
        "check for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIfpdqRg_gS9",
        "outputId": "f8be6657-9406-4a2f-fe7e-c1b94bfa9127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7470\n"
          ]
        }
      ],
      "source": [
        "sdf = sdf.drop_duplicates(subset=['lesion_id']) \n",
        "sdf = sdf.drop_duplicates(subset=['image_id']) \n",
        "\n",
        "print(sdf.shape[0]) #now i have 7470 lesions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtDyJ-AlEqHy"
      },
      "source": [
        "check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5htrKrBD_kh_",
        "outputId": "ab8873d6-48f8-457b-ace0-25f04793c038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of non-NA data per columns:\n",
            " lesion_id        0\n",
            "image_id         0\n",
            "dx               0\n",
            "dx_type          0\n",
            "age             52\n",
            "sex              0\n",
            "localization     0\n",
            "path             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"number of non-NA data per columns:\\n\",sdf.isnull().sum())\n",
        "#some NA in age -> since that column will be removed from the dataset, i don't impute them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No8M26AlEwHj"
      },
      "source": [
        "## 1.3 - Create the binary label:\n",
        "\n",
        "\n",
        "*   **M**: If the lesion is Malignant\n",
        "*   **B**: If the lesion is Benign\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chdaPX_F_leK"
      },
      "outputs": [],
      "source": [
        "#Define Benign and Malignant lesions\n",
        "Benign = [\"nv\", \"bkl\", \"vasc\", \"df\"]\n",
        "Malignant = [\"mel\", \"bcc\", \"akiec\"]\n",
        "m1 = sdf['dx'].isin(Benign)\n",
        "m2 = sdf['dx'].isin(Malignant)\n",
        "sdf['type'] = np.select([m1, m2], [\"B\", \"M\"], default=sdf['dx'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFwrVxcUFLo_"
      },
      "source": [
        "# **2 - Data Visualisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCQxeeBcFU4S"
      },
      "source": [
        "Malignant\\Benign Distribution\n",
        "\n",
        "* **Green** if the lesion is **Benign**\n",
        "\n",
        "* **Red** if the lesion is **Malignant**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEU5scfUFebT"
      },
      "outputs": [],
      "source": [
        "binclass_perc = round((sdf[\"type\"].value_counts() / len(sdf[\"type\"])), 2)\n",
        "#B    0.84\n",
        "#M    0.16\n",
        "plt.bar(binclass_perc.index, binclass_perc, color = [\"green\", \"red\"])\n",
        "\n",
        "#I observe that the 2 classes are unbalanced --> SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxCr0g8PFyON"
      },
      "source": [
        "# **3 - Data Manipulation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ato3qO79FZ1"
      },
      "source": [
        "## 3.1 - Generate the Training (train + val) and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pxRdQi28Ruk"
      },
      "source": [
        "Encode the response variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdsnQGq_5ESD"
      },
      "outputs": [],
      "source": [
        "sdf['label'] = pd.Categorical(sdf['type']).codes\n",
        "sdf = sdf[['path', 'label']]\n",
        "\n",
        "#this function, defined in the section 'functions', fix the row-indeces of the dataset\n",
        "#since the download (and in the next steps the sampling), save them in a unconvinient manner\n",
        "sdf = re_index(sdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1-sb_tP9QbC"
      },
      "source": [
        "sample the training and the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AszEOHyn5GCK"
      },
      "outputs": [],
      "source": [
        "#training (train + val)\n",
        "train_val = sdf.sample(frac=0.7, random_state=19)\n",
        "##need to convert it in an array to merge it in the future with the augumented data\n",
        "x_train_val = train_val[['path']].to_numpy() \n",
        "\n",
        "#test\n",
        "test = sdf.drop(train_val.index)\n",
        "\n",
        "#re index the df\n",
        "train_val = re_index(train_val)\n",
        "test = re_index(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08RuVDk9u9P"
      },
      "source": [
        "## 3.2 - Store the images as arrays (Expected time: 3m)\n",
        " \n",
        " The images are sized as 32x32 since speed up the computation of the algorithm mantaining a good amount of information\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6GTlQ995J4C"
      },
      "outputs": [],
      "source": [
        "sh_x, sh_y = 32, 32\n",
        "train_val['image'] = train_val['path'].map(lambda x: np.asarray(Image.open(x).resize((sh_x,sh_y))))\n",
        "test['image'] = test['path'].map(lambda x: np.asarray(Image.open(x).resize((sh_x,sh_y))))\n",
        "\n",
        "train_val = train_val[['image', 'label']]\n",
        "x_train_val = train_val[['image']] \n",
        "test = test[['image', 'label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNaWXYlg-a3G"
      },
      "source": [
        "## 3.3 - Data Augumentation\n",
        "\n",
        "Since the training set contain around 5000 observations, a good practice is to generate synthetic images in order to increase the number of observation. \n",
        "\n",
        "I have a randomized mix of affine trasformation, vertical and horizontal flip.\n",
        "\n",
        "This mix it has been applied to each image, so that the dataset cardinality is doubled\n",
        "\n",
        "**Note that** the augumented data have been used only in the training set (train +validation), while in the test set has been used only original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdNZ83A75LCl"
      },
      "outputs": [],
      "source": [
        "aug = list()\n",
        "torch.manual_seed(19)\n",
        "transforms = T.RandomApply(torch.nn.ModuleList([T.RandomVerticalFlip(p=1.0), \\\n",
        "                                                T.RandomHorizontalFlip(p=1.0)]), p=1)\n",
        "\n",
        "#augumenter = T.RandomPerspective(distortion_scale=0.6, p=1.0)\n",
        "aug = train_val[\"image\"].map(lambda x: transforms(torch.tensor(x).permute(2,0,1)))\n",
        "\n",
        "for i in range(len(aug)):\n",
        "  aug[i] = np.array(aug[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26sJrb8eAKq7"
      },
      "source": [
        "Create the dataset containing the original data and the augumented ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6eeeXa45PRZ"
      },
      "outputs": [],
      "source": [
        "#trasform the augumented data as an array\n",
        "aug = np.asarray(aug.tolist())\n",
        "aug = aug.reshape(aug.shape[0], *(sh_x, sh_y, 3))\n",
        "\n",
        "#trasform the images into array\n",
        "x_train_val = np.asarray(x_train_val.values.tolist())\n",
        "x_train_val= x_train_val.reshape(x_train_val.shape[0], *(sh_x, sh_y, 3))\n",
        "\n",
        "#concatenete the 2 arrays\n",
        "df_images = np.concatenate((aug, x_train_val))\n",
        "\n",
        "#double the labels to make them fit with the original data and the concatenated augumented data\n",
        "df_labels = np.concatenate((train_val[['label']].to_numpy(),train_val[['label']].to_numpy() ))\n",
        "\n",
        "#define the dataset containing training and val examples\n",
        "coln1 = {'image': list(df_images), 'label': list(df_labels)}\n",
        "trainval = pd.DataFrame(data=coln1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5y18De3-MOjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNKCoLdTA6yE"
      },
      "source": [
        "## 3.4 - Split the training set in Train and Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XubzKXVG5RS_",
        "outputId": "58cf3422-5f91-4e02-e9c6-0dcdd608d2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 7320 \n",
            "val: 3138\n"
          ]
        }
      ],
      "source": [
        "train, val = train_test_split(trainval, test_size=0.30,random_state=19)\n",
        "#fix the indexes\n",
        "train = re_index(train)\n",
        "val = re_index(val)\n",
        "print(\"train:\", len(train), \"\\nval:\", len(val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJp2zVZ7BuUy"
      },
      "source": [
        "## 3.5 - Define features and response variable for train, validation and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uavM1nqz5Tp3"
      },
      "outputs": [],
      "source": [
        "#train\n",
        "x_train = train.drop([\"label\"], axis = 1)\n",
        "x_train = np.asarray(x_train['image'].tolist())\n",
        "y_train = train[\"label\"]\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "#validation\n",
        "x_val = val.drop([\"label\"], axis = 1)\n",
        "x_val = np.asarray(x_val['image'].tolist())\n",
        "y_val = val[\"label\"]\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "#test\n",
        "x_test = test.drop([\"label\"], axis = 1)\n",
        "x_test = np.asarray(x_test['image'].tolist())\n",
        "y_test = test[\"label\"]\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtNOohOXB_Pf"
      },
      "source": [
        "## 3.6 - SMOTE\n",
        "\n",
        "As noticed before, the data are strongly umbalanced. SMOTE is an oversampling technique which balance the classes by generate syntethic observations of the less represented class.\n",
        "\n",
        "**Note that** only the data contained in the train set have been oversampled in this way, since in the real word the data are no evenly distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIuxA2g-5XIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1edf48-e85c-4e74-9e32-8659d91aab32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: 12328\n"
          ]
        }
      ],
      "source": [
        "#Smote\n",
        "x_train = x_train.reshape(x_train.shape[0], sh_x * sh_y* 3)\n",
        "y_train= y_train.astype('int')\n",
        "sm = SMOTE(random_state=42)\n",
        "x_train, y_train = sm.fit_resample(x_train, y_train)\n",
        "\n",
        "print(\"x_train:\", len(x_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv6WFjGCC_cs"
      },
      "source": [
        "## 3.7 - Reshape and convert the images in the correct format\n",
        "\n",
        "all the images have been shaped as 32x32x3 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72gCZO3h5Z65"
      },
      "outputs": [],
      "source": [
        "#reshape to be sure that they are in the right shape\n",
        "x_train = x_train.reshape(x_train.shape[0], *(sh_x, sh_y, 3))\n",
        "x_val = x_val.reshape(x_val.shape[0], *(sh_x, sh_y, 3))\n",
        "x_test = x_test.reshape(x_test.shape[0], *(sh_x, sh_y, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZQRx_7q5eWO"
      },
      "outputs": [],
      "source": [
        "# converting the images in torch format\n",
        "x_train  =torch.from_numpy(x_train.astype(np.float32))\n",
        "x_val  = torch.from_numpy(x_val.astype(np.float32))\n",
        "x_test  = torch.from_numpy(x_test.astype(np.float32))\n",
        "\n",
        "#converting the lables in torch format\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_train = y_train.unsqueeze(1)\n",
        "\n",
        "y_val = torch.from_numpy(y_val.astype(np.float32))\n",
        "y_val = y_val.unsqueeze(1)\n",
        "\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "y_test = y_test.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8 - Normalization \n",
        "\n",
        "All the images will be normalized based on the mean and std dev of the train set\n",
        "\n"
      ],
      "metadata": {
        "id": "bBDGOl29zBZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = TF.normalize(x_train, x_train.mean(), x_train.std())\n",
        "x_val = TF.normalize(x_val, x_val.mean(), x_val.std())\n",
        "x_test = TF.normalize(x_test, x_test.mean(), x_test.std())"
      ],
      "metadata": {
        "id": "unAxuJ8_zSoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crfjmSQpOKqA"
      },
      "source": [
        "# **4 - Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNVmSe_OOdta"
      },
      "source": [
        "## 4.1 Pytorch Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OAyqmQROaB2"
      },
      "source": [
        "use GPU if possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP4f6B1AOSLS",
        "outputId": "99e02507-8261-44b3-9702-e14d8d0d04d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kLTojVuFb0z"
      },
      "source": [
        "Define the class Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-LUhln2OVw2"
      },
      "outputs": [],
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.permute(0,3,1,2)\n",
        "        self.y = y.type(torch.LongTensor)\n",
        "        self.len = self.X.shape[0]\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "   \n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoyLzTbKOgGf"
      },
      "source": [
        "Define the Train, Validation and Test Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdaLhh1yOlIq"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 \n",
        "\n",
        "train_set = Data(x_train, y_train)\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_set = Data(x_val, y_val)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_set = Data(x_test, y_test)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymOkAicwOrvf"
      },
      "source": [
        "## 4.2 Binary Model 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUCZeo-ncMVp"
      },
      "outputs": [],
      "source": [
        "class Binary_1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.drop50 = nn.Dropout(0.20)\n",
        "        self.drop30 = nn.Dropout(0.20) \n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(3, 3))\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        \n",
        "        self.linear1 = nn.Linear(1152, 64)\n",
        "        self.norm1d1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.linear2 = nn.Linear(64, 32)\n",
        "        self.norm1d2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.linear3 = nn.Linear(32, 16)\n",
        "        self.norm1d3 = nn.BatchNorm1d(16)\n",
        "\n",
        "        self.linear4= nn.Linear(16, 1)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "  \n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.norm2d1(x)\n",
        "        x = self.drop30(x)\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.norm2d2(x)\n",
        "        x = self.drop30(x)\n",
        "\n",
        "        x = self.ReLU(self.conv3(x))\n",
        "        x = self.norm2d3(x)\n",
        "        x = self.drop30(x)\n",
        "\n",
        "        x = self.ReLU(self.conv4(x))\n",
        "        x = self.norm2d4(x)\n",
        "        x = self.drop30(x)\n",
        "\n",
        "        x = self.flat(x)\n",
        "\n",
        "        x = self.ReLU(self.linear1(x))\n",
        "        x = self.norm1d1(x)\n",
        "        x = self.drop50(x)\n",
        "\n",
        "        x = self.ReLU(self.linear2(x))\n",
        "        x = self.norm1d2(x)\n",
        "        x = self.drop50(x)\n",
        "\n",
        "        x = self.ReLU(self.linear3(x))\n",
        "        x = self.norm1d3(x)\n",
        "        x = self.drop50(x)\n",
        "\n",
        "        x = self.sigmoid(self.linear4(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXW7Id7qeD_N"
      },
      "source": [
        "## 4.3 - Binary Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWjpRFa_M6Nm"
      },
      "outputs": [],
      "source": [
        "class Binary_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.drop = nn.Dropout(0.30)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(3, 3))\n",
        "        self.Sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm1d2 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        \n",
        "        self.linear1 = nn.Linear(1152, 32)\n",
        "        self.norm1d1 = nn.BatchNorm1d(32)\n",
        "        self.linear2 = nn.Linear(32, 1)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "  \n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.norm1d2(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.norm2d2(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.flat(x)\n",
        "\n",
        "        x = self.ReLU(self.linear1(x))\n",
        "        x = self.norm1d1(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.Sigmoid(self.linear2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN0_KKr0F7O6"
      },
      "source": [
        "## 4.4 - Train the model "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1"
      ],
      "metadata": {
        "id": "awL_J_V8KG1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy: 0.731 \n",
        "n_epochs = 50\n",
        "model1 = Binary_1()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model1.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    factor=0.1, \n",
        "    patience=10, \n",
        "    verbose=True,\n",
        "    min_lr = 0.000001)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "loss1, acc1 = Prediction(model1, train_loader, val_loader, n_epochs, criterion, optimizer, show_progress = True)"
      ],
      "metadata": {
        "id": "Rs42aQsSdWm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2"
      ],
      "metadata": {
        "id": "wuxxj9z9KIxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip8y88SiFIe9"
      },
      "outputs": [],
      "source": [
        "#Accuracy: 0.673\n",
        "n_epochs = 100\n",
        "model2 = Binary_2()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model2.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    factor=0.1, \n",
        "    patience=5, \n",
        "    verbose=False,\n",
        "    min_lr = 0.000001)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "loss2, acc2 = Prediction(model2, train_loader, val_loader, n_epochs, criterion, optimizer, show_progress = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n"
      ],
      "metadata": {
        "id": "nxOVshWcBkwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#summary(Binary_2(), (3,32,32))"
      ],
      "metadata": {
        "id": "CZ0kt46vBmYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frPpiZ7yGQE9"
      },
      "source": [
        "## 4.5 - Results visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "UbtkXuNUW9Hm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnPh15soGOiA"
      },
      "outputs": [],
      "source": [
        "plt.plot(acc1[\"training_accuracy\"], label = \"training accuracy\")\n",
        "plt.plot(acc1[\"validation_accuracy\"], label = \"validation accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss"
      ],
      "metadata": {
        "id": "HVruDJirXABx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss1[\"training_loss\"], label = \"training loss\")\n",
        "plt.plot(loss1[\"validation_loss\"], label = \"validation loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss\")\n"
      ],
      "metadata": {
        "id": "czDZqPFmXBkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model 2\n"
      ],
      "metadata": {
        "id": "In6_Rf5pbV63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss2[\"training_loss\"], label = \"training loss\")\n",
        "plt.plot(loss2[\"validation_loss\"], label = \"validation loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss\")"
      ],
      "metadata": {
        "id": "PlLhdk2rbWK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(acc2[\"training_accuracy\"], label = \"training accuracy\")\n",
        "plt.plot(acc2[\"validation_accuracy\"], label = \"validation accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy\")\n"
      ],
      "metadata": {
        "id": "C6ix9nwMbTjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVESoNWAiu_r"
      },
      "source": [
        "## 4.6 - Prediction on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy1L6q6XhNKH"
      },
      "outputs": [],
      "source": [
        "preds = test_prediction(model1, test_loader)  #model2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVBjTYgz4qvE"
      },
      "source": [
        "trasform the data in order to build the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYtS0sgjY-ZH"
      },
      "outputs": [],
      "source": [
        "#Need to store the data in a proper manner to visualize the metrics of interest\n",
        "y_pred_list = [a.squeeze().tolist() for a in preds]\n",
        "\n",
        "#rest = len(y_test) % batch_size\n",
        "#last batch has only one element --> i temporary remove it to apply vstack.\n",
        "y_pred_list1 = y_pred_list[:-1]\n",
        "last_element = y_pred_list[-1]\n",
        "\n",
        "p = np.vstack(y_pred_list1)\n",
        "ytest_pred = list(itertools.chain.from_iterable(p))\n",
        "\n",
        "#append the last element\n",
        "ytest_pred.insert(len(ytest_pred), last_element)\n",
        "\n",
        "\n",
        "y_true_test = y_test.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXU0HjVO4yma"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rliSXs8mgObV"
      },
      "outputs": [],
      "source": [
        "a = conf_matrix = confusion_matrix(y_true_test, ytest_pred)\n",
        "\n",
        "adf = pd.DataFrame(a, columns = ['Pred_Benign', 'Pred_Malignant'])\n",
        "adf.index = ['Actual_Benign', 'Actual_Malignant']\n",
        "sns.heatmap(adf, annot=True, fmt = 'g', cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjFIVywY44b3"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS9UNTqQgZ5V"
      },
      "outputs": [],
      "source": [
        "Accuracy = round((a[0][0] + a[1][1]) / sum(sum(a)), 3)\n",
        "print(\"Accuracy:\", Accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uynDjn0W39-c"
      },
      "source": [
        "# **5 - Dropout Tuning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 - Define the model\n",
        "\n",
        "Based on Model 1 structure"
      ],
      "metadata": {
        "id": "5JKV1c1R3tlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjizXNXiw1N7"
      },
      "outputs": [],
      "source": [
        "class Binary_Tune(nn.Module):\n",
        "    def __init__(self, drp):\n",
        "        super().__init__()\n",
        "        self.dr = drp \n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.drop = nn.Dropout(drp)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(3, 3))\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3,3), padding = \"same\")\n",
        "        self.norm2d4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        \n",
        "        self.linear1 = nn.Linear(1152, 64)\n",
        "        self.norm1d1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.linear2 = nn.Linear(64, 32)\n",
        "        self.norm1d2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.linear3 = nn.Linear(32, 16)\n",
        "        self.norm1d3 = nn.BatchNorm1d(16)\n",
        "\n",
        "        self.linear4= nn.Linear(16, 1)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "  \n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.norm2d1(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.norm2d2(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.ReLU(self.conv3(x))\n",
        "        x = self.norm2d3(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.ReLU(self.conv4(x))\n",
        "        x = self.norm2d4(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.flat(x)\n",
        "\n",
        "        x = self.ReLU(self.linear1(x))\n",
        "        x = self.norm1d1(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.ReLU(self.linear2(x))\n",
        "        x = self.norm1d2(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.ReLU(self.linear3(x))\n",
        "        x = self.norm1d3(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.sigmoid(self.linear4(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 - Tuning\n",
        "\n",
        "The idea is to try different values of dropout in the model, since it one of the mai causes of overfitting, and than choose the model which minimize the average square difference between the training and the validation accuracy.\n",
        "\n",
        "The dropout values tried go from 0% to 50% with step 5."
      ],
      "metadata": {
        "id": "xwcX4IQV3yqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#50 epochs to speed up the computation\n",
        "n_epochs = 50\n",
        "#list of the dropouts\n",
        "tune = list()\n",
        "\n",
        "for i in tqdm_notebook(range(1, 11)): \n",
        "  drp = i*0.05\n",
        "  model_tune = None\n",
        "  model_tune = Binary_Tune(drp = drp)\n",
        "  learning_rate = 0.1\n",
        "  optimizer = torch.optim.SGD(model_tune.parameters(), lr=learning_rate)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer, \n",
        "      factor=0.1, \n",
        "      patience=10, \n",
        "      verbose=False,\n",
        "      min_lr = 0.000001)\n",
        "  \n",
        "  #derive val and train loss\n",
        "  accT = Prediction(model_tune, train_loader, val_loader, n_epochs, criterion, optimizer, show_progress = False)[1]\n",
        "\n",
        "  #find the avg square distance\n",
        "  l1 = accT.get(\"training_accuracy\")\n",
        "  l2 = accT.get(\"validation_accuracy\")\n",
        "  #multiply to have them (usually) > 1\n",
        "  diff = (np.asarray(l1) - np.asarray(l2))*100\n",
        "\n",
        "  #median of the square\n",
        "  median_result = np.median(np.square(diff))\n",
        "\n",
        "  print('[model: %d ==> Median accuracy difference %f:' % (i, median_result))\n",
        "  print(\"---------------------------------\")\n",
        "\n",
        "  #update the list\n",
        "  tune.append(median_result)\n"
      ],
      "metadata": {
        "id": "8DNGGQMgjVDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot of dropouts"
      ],
      "metadata": {
        "id": "2-MCF3qXpcD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
        "ax.plot(tune)\n",
        "#ax.set_xticks([0,2,4,6,8,10])\n",
        "ax.set_xticklabels([round(x*0.05,2) for x in range(2,11)])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9chAWwTMUmcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best dropout"
      ],
      "metadata": {
        "id": "o59Auby7ChPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_dropout = round((tune.index(min(tune)))*0.05, 2)\n",
        "best_dropout"
      ],
      "metadata": {
        "id": "rqRplWed00jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-train the best model"
      ],
      "metadata": {
        "id": "RjIa1ltPpIMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = Binary_Tune(drp = best_dropout)\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(best_model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    factor=0.1, \n",
        "    patience=10, \n",
        "    verbose=False,\n",
        "    min_lr = 0.000001)\n",
        "\n",
        "#derive val and train loss\n",
        "loss_best, acc_best = Prediction(best_model, train_loader, val_loader, n_epochs, criterion, optimizer, show_progress = False)\n"
      ],
      "metadata": {
        "id": "C4JyUB6Fo1tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 - Results on test set"
      ],
      "metadata": {
        "id": "d5rnaOGCCqgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = test_prediction(best_model, test_loader)"
      ],
      "metadata": {
        "id": "wbzsrLQeCrK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to store the data in a proper manner to visualize the metrics of interest\n",
        "y_pred_list = [a.squeeze().tolist() for a in preds]\n",
        "\n",
        "#rest = len(y_test) % batch_size\n",
        "#last batch has only one element --> i temporary remove it to apply vstack.\n",
        "y_pred_list1 = y_pred_list[:-1]\n",
        "last_element = y_pred_list[-1]\n",
        "\n",
        "p = np.vstack(y_pred_list1)\n",
        "ytest_pred = list(itertools.chain.from_iterable(p))\n",
        "\n",
        "#append the last element\n",
        "ytest_pred.insert(len(ytest_pred), last_element)\n",
        "\n",
        "\n",
        "y_true_test = y_test.ravel()\n",
        "\n",
        "a = conf_matrix = confusion_matrix(y_true_test, ytest_pred)\n",
        "\n",
        "adf = pd.DataFrame(a, columns = ['Pred_Benign', 'Pred_Malignant'])\n",
        "adf.index = ['Actual_Benign', 'Actual_Malignant']\n",
        "sns.heatmap(adf, annot=True, fmt = 'g', cmap=\"Blues\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0MHo8c-A36j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy = round((a[0][0] + a[1][1]) / sum(sum(a)), 3)\n",
        "print(\"Accuracy:\", Accuracy)"
      ],
      "metadata": {
        "id": "DmErSCPKLGon"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CTwd-q_DDg0m",
        "c6cncw0HDlx0",
        "7fS1sJvrDo38",
        "2DGUb17oECQ_",
        "lQfxsIBCEIje",
        "tLnL3_d6EPKR",
        "AE5m2sGVEg6O",
        "No8M26AlEwHj",
        "XFwrVxcUFLo_",
        "NxCr0g8PFyON",
        "4ato3qO79FZ1",
        "f08RuVDk9u9P",
        "aNaWXYlg-a3G",
        "SNKCoLdTA6yE",
        "cJp2zVZ7BuUy",
        "AtNOohOXB_Pf",
        "bv6WFjGCC_cs",
        "bBDGOl29zBZG",
        "crfjmSQpOKqA",
        "FNVmSe_OOdta",
        "ymOkAicwOrvf",
        "rXW7Id7qeD_N",
        "YN0_KKr0F7O6",
        "frPpiZ7yGQE9",
        "ZVESoNWAiu_r",
        "uynDjn0W39-c",
        "5JKV1c1R3tlE",
        "xwcX4IQV3yqm",
        "d5rnaOGCCqgb"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPpDreGvOGCEP4cuu2TVm4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}